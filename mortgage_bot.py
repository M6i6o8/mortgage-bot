"""
–ò–ø–æ—Ç–µ—á–Ω—ã–π –±–æ—Ç - –ê–ù–¢–ò–ë–ê–ù-–í–ï–†–°–ò–Ø —Å 10 –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
–ó–∞–ø—É—Å–∫ –Ω–∞ GitHub Actions
"""

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import os
import random
import time
import warnings

warnings.filterwarnings("ignore", category=DeprecationWarning)

# ===== –ù–ê–°–¢–†–û–ô–ö–ò =====
BOT_TOKEN = os.environ.get('BOT_TOKEN', '')
CHANNEL_ID = os.environ.get('CHANNEL_ID', '')

# ===== –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ü–†–û–ö–°–ò-–ú–ï–ù–ï–î–ñ–ï–† =====
class ProxyManager:
    def __init__(self):
        self.http_proxies = []
        self.socks_proxies = []
        self.current_proxy = None
        self.update_all_proxies()
    
    def fetch_proxies(self, url, proxy_type='http'):
        try:
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                proxies = [line.strip() for line in response.text.strip().split('\n') if line.strip()]
                return proxies
            return []
        except:
            return []
    
    def update_all_proxies(self):
        """–ö–∞—á–∞–µ—Ç HTTP –∏ SOCKS –ø—Ä–æ–∫—Å–∏ –∏–∑ 8 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        print("  –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–∫—Å–∏ (HTTP+SOCKS)...")
        all_http = []
        all_socks = []
        
        # HTTP –ø—Ä–æ–∫—Å–∏ (8 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)
        http_sources = [
            "https://api.proxyscrape.com/v2/?request=displayproxies&protocol=http&timeout=10000&country=all&ssl=all",
            "https://raw.githubusercontent.com/GoekhanDev/free-proxy-list/main/http.txt",
            "https://raw.githubusercontent.com/proxifly/free-proxy-list/main/proxies/protocols/http/data.txt",
            "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
            "https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt",
            "https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies-http.txt",
            "https://raw.githubusercontent.com/officialputuid/KangProxy/KangProxy/http/http.txt",
            "https://raw.githubusercontent.com/roosterkid/openproxylist/main/HTTP_RAW.txt",
        ]
        
        for url in http_sources:
            proxies = self.fetch_proxies(url)
            all_http.extend(proxies)
            print(f"    HTTP –∏—Å—Ç–æ—á–Ω–∏–∫: +{len(proxies)}")
        
        # SOCKS5 –ø—Ä–æ–∫—Å–∏ (–¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–∞–π—Ç–æ–≤)
        socks_sources = [
            "https://api.proxyscrape.com/v2/?request=displayproxies&protocol=socks5&timeout=10000&country=all",
            "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks5.txt",
            "https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/socks5.txt",
            "https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies-socks5.txt",
        ]
        
        for url in socks_sources:
            proxies = self.fetch_proxies(url)
            all_socks.extend(proxies)
            print(f"    SOCKS –∏—Å—Ç–æ—á–Ω–∏–∫: +{len(proxies)}")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        self.http_proxies = list(set(all_http))[:100]
        self.socks_proxies = list(set(all_socks))[:50]
        print(f"    ‚úÖ HTTP –ø—Ä–æ–∫—Å–∏: {len(self.http_proxies)}")
        print(f"    ‚úÖ SOCKS –ø—Ä–æ–∫—Å–∏: {len(self.socks_proxies)}")
    
    def get_http_proxy(self):
        if not self.http_proxies:
            self.update_all_proxies()
        
        if self.http_proxies:
            proxy = random.choice(self.http_proxies)
            return {
                'http': f'http://{proxy}',
                'https': f'http://{proxy}'
            }
        return None
    
    def get_socks_proxy(self):
        if not self.socks_proxies:
            self.update_all_proxies()
        
        if self.socks_proxies:
            proxy = random.choice(self.socks_proxies)
            return {
                'http': f'socks5://{proxy}',
                'https': f'socks5://{proxy}'
            }
        return None

# ===== –ë–†–ê–£–ó–ï–†–ù–´–ô –≠–ú–£–õ–Ø–¢–û–† =====
class BrowserEmulator:
    def __init__(self):
        self.session = requests.Session()
        self.setup_session()
    
    def setup_session(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é –∫–∞–∫ —Ä–µ–∞–ª—å–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä"""
        # –ö—É–∫–∏ –∫–∞–∫ —É Chrome
        self.session.cookies.set('_ym_uid', str(random.randint(1000000, 9999999)))
        self.session.cookies.set('_ym_d', str(int(time.time())))
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        })
    
    def get_headers(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º User-Agent"""
        ua_list = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1',
        ]
        headers = self.session.headers.copy()
        headers['User-Agent'] = random.choice(ua_list)
        return headers

# ===== –ü–ê–†–°–ï–† =====
class MegaParser:
    def __init__(self):
        self.all_rates = {}
        self.proxy_manager = ProxyManager()
        self.browser = BrowserEmulator()
        
        # –†–µ–≥—É–ª—è—Ä–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–∞–Ω–∫–æ–≤ –∏ —Å—Ç–∞–≤–æ–∫
        self.bank_patterns = [
            r'([–ê-–Ø][–∞-—è]+(?:\s+[–ê-–Ø][–∞-—è]+)*(?:\s+–±–∞–Ω–∫)?)',
            r'(–°–±–µ—Ä|–í–¢–ë|–ê–ª—å—Ñ–∞|–¢-–ë–∞–Ω–∫|–ì–∞–∑–ø—Ä–æ–º|–†–æ—Å—Å–µ–ª—å—Ö–æ–∑|–ü—Ä–æ–º—Å–≤—è–∑—å|–£—Ä–∞–ª—Å–∏–±|–û—Ç–∫—Ä—ã—Ç–∏–µ|–°–æ–≤–∫–æ–º|–ú–¢–°)',
        ]
        
        self.rate_patterns = [
            r'–æ—Ç\s*(\d+[.,]\d+)%',
            r'(\d+[.,]\d+)%\s*–≥–æ–¥–æ–≤—ã—Ö',
            r'(\d+[.,]\d+)%',
            r'—Å—Ç–∞–≤–∫–∞[^\d]*(\d+[.,]\d+)',
        ]
    
    def extract_banks(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –±–∞–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        banks = []
        for pattern in self.bank_patterns:
            matches = re.findall(pattern, text)
            banks.extend(matches)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º
        filtered = []
        for bank in banks:
            bank = bank.strip()
            if len(bank) > 3 and len(bank) < 30 and not any(x in bank.lower() for x in ['—Ä—É–±', '–≥–æ–¥', '–º–µ—Å', '—Å—É–º–º–∞']):
                filtered.append(bank)
        
        return list(set(filtered))
    
    def extract_rate(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ-—Å—Ç–∞–≤–∫—É –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return None
        
        for pattern in self.rate_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                rate_str = match.group(1).replace(',', '.')
                try:
                    rate = float(rate_str)
                    if 5 <= rate <= 35:
                        return rate
                except:
                    continue
        return None
    
    def parse_with_retry(self, url, use_proxy=True, use_socks=False, retries=3):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å —Ä–µ—Ç—Ä–∞—è–º–∏"""
        
        for attempt in range(retries):
            try:
                # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–∫—Å–∏
                proxies = None
                if use_proxy:
                    if use_socks:
                        proxies = self.proxy_manager.get_socks_proxy()
                    else:
                        proxies = self.proxy_manager.get_http_proxy()
                
                # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
                session = requests.Session()
                if proxies:
                    session.proxies.update(proxies)
                
                headers = self.browser.get_headers()
                
                # –ó–∞—Ö–æ–¥–∏–º –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å–Ω–∞—á–∞–ª–∞ (–¥–ª—è –∫—É–∫)
                if 'banki.ru' in url:
                    session.get('https://www.banki.ru/', headers=headers, timeout=10)
                    time.sleep(1)
                
                response = session.get(url, headers=headers, timeout=15)
                
                if response.status_code == 200:
                    return response.text
                else:
                    print(f"      –°—Ç–∞—Ç—É—Å {response.status_code}, –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞...")
                    
            except Exception as e:
                print(f"      –û—à–∏–±–∫–∞: {str(e)[:50]}...")
            
            time.sleep(2)
        
        return None
    
    # ===== –ò–°–¢–û–ß–ù–ò–ö 1: –ë–∞–Ω–∫–∏.—Ä—É =====
    def parse_banki_ru(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ë–∞–Ω–∫–∏.—Ä—É —á–µ—Ä–µ–∑ SOCKS5 –ø—Ä–æ–∫—Å–∏"""
        print("  [1/10] –ë–∞–Ω–∫–∏.—Ä—É...")
        url = "https://www.banki.ru/products/ipoteka/"
        
        html = self.parse_with_retry(url, use_proxy=True, use_socks=True, retries=5)
        if not html:
            print("    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return 0
        
        soup = BeautifulSoup(html, 'html.parser')
        found = 0
        
        # –ò—â–µ–º –±–∞–Ω–∫–∏ –ø–æ —Ä–∞–∑–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        selectors = [
            {'bank': ['a', {'data-test': 'name'}], 'rate': ['span', {'data-test': 'rate'}]},
            {'bank': ['span', {'class': 'font-bold'}], 'rate': ['span', {'class': 'font-bold'}]},
            {'bank': ['td', {'class': 'name'}], 'rate': ['td', {'class': 'rate'}]},
        ]
        
        for selector in selectors:
            bank_tags = soup.find_all(selector['bank'][0], selector['bank'][1])
            for tag in bank_tags[:20]:
                try:
                    bank_name = tag.get_text().strip()
                    parent = tag.find_parent('tr')
                    if parent:
                        rate_text = parent.get_text()
                        rate = self.extract_rate(rate_text)
                        
                        if bank_name and rate:
                            self.all_rates[bank_name] = rate
                            found += 1
                            print(f"      ‚úì {bank_name[:20]}: {rate}%")
                except:
                    continue
        
        if found > 0:
            print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {found}")
        return found
    
    # ===== –ò–°–¢–û–ß–ù–ò–ö 2: –°—Ä–∞–≤–Ω–∏.—Ä—É =====
    def parse_sravni_ru(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –°—Ä–∞–≤–Ω–∏.—Ä—É"""
        print("  [2/10] –°—Ä–∞–≤–Ω–∏.—Ä—É...")
        url = "https://www.sravni.ru/ipoteka/"
        
        html = self.parse_with_retry(url, use_proxy=False)
        if not html:
            print("    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return 0
        
        soup = BeautifulSoup(html, 'html.parser')
        found = 0
        
        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏
        cards = soup.find_all('div', class_=re.compile('ProductCard|BankCard|Offer'))
        
        for card in cards[:20]:
            try:
                text = card.get_text()
                
                # –ò—â–µ–º –±–∞–Ω–∫
                banks = self.extract_banks(text)
                if not banks:
                    continue
                
                bank_name = banks[0]
                rate = self.extract_rate(text)
                
                if bank_name and rate:
                    self.all_rates[bank_name] = rate
                    found += 1
                    print(f"      ‚úì {bank_name[:20]}: {rate}%")
                    
            except:
                continue
        
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {found}")
        return found
    
    # ===== –ò–°–¢–û–ß–ù–ò–ö 3: –ú–ë–ö =====
    def parse_mbk_ru(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ú–ë–ö"""
        print("  [3/10] –ú–ë–ö...")
        url = "https://www.mbk.ru/ipoteka/"
        
        html = self.parse_with_retry(url, use_proxy=False)
        if not html:
            print("    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return 0
        
        soup = BeautifulSoup(html, 'html.parser')
        found = 0
        
        blocks = soup.find_all('div', class_=re.compile('bank-item|product-card|item'))
        
        for block in blocks[:15]:
            try:
                text = block.get_text()
                
                banks = self.extract_banks(text)
                if not banks:
                    continue
                
                bank_name = banks[0]
                rate = self.extract_rate(text)
                
                if bank_name and rate:
                    self.all_rates[bank_name] = rate
                    found += 1
                    print(f"      ‚úì {bank_name[:20]}: {rate}%")
                    
            except:
                continue
        
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {found}")
        return found
    
    # ===== –ò–°–¢–û–ß–ù–ò–ö 4: –í—ã–±–µ—Ä—É.—Ä—É =====
    def parse_vbr_ru(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –í—ã–±–µ—Ä—É.—Ä—É"""
        print("  [4/10] –í—ã–±–µ—Ä—É.—Ä—É...")
        url = "https://www.vbr.ru/banki/ipoteka/"
        
        html = self.parse_with_retry(url, use_proxy=False)
        if not html:
            print("    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return 0
        
        soup = BeautifulSoup(html, 'html.parser')
        found = 0
        
        items = soup.find_all('div', class_=re.compile('b-list-item|product-item'))
        
        for item in items[:15]:
            try:
                text = item.get_text()
                
                banks = self.extract_banks(text)
                if not banks:
                    continue
                
                bank_name = banks[0]
                rate = self.extract_rate(text)
                
                if bank_name and rate:
                    self.all_rates[bank_name] = rate
                    found += 1
                    print(f"      ‚úì {bank_name[:20]}: {rate}%")
                    
            except:
                continue
        
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {found}")
        return found
    
    # ===== –ò–°–¢–û–ß–ù–ò–ö 5: –§–∏–Ω—É—Å–ª—É–≥–∏ =====
    def parse_finuslugi_ru(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –§–∏–Ω—É—Å–ª—É–≥–∏"""
        print("  [5/10] –§–∏–Ω—É—Å–ª—É–≥–∏...")
        url = "https://finuslugi.ru/mortgages"
        
        html = self.parse_with_retry(url, use_proxy=False)
        if not html:
            print("    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return 0
        
        soup = BeautifulSoup(html, 'html.parser')
        found = 0
        
        cards = soup.find_all('div', class_=re.compile('card|product|item'))
        
        for card in cards[:15]:
            try:
                text = card.get_text()
                
                banks = self.extract_banks(text)
                if not banks:
                    continue
                
                bank_name = banks[0]
                rate = self.extract_rate(text)
                
                if bank_name and rate:
                    self.all_rates[bank_name] = rate
                    found += 1
                    print(f"      ‚úì {bank_name[:20]}: {rate}%")
                    
            except:
                continue
        
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {found}")
        return found
    
    # ===== –ò–°–¢–û–ß–ù–ò–ö 6: –ë–∞–Ω–∫–ò–Ω—Ñ–æ—Ä–º =====
    def parse_bankinform_ru(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ë–∞–Ω–∫–ò–Ω—Ñ–æ—Ä–º"""
        print("  [6/10] –ë–∞–Ω–∫–ò–Ω—Ñ–æ—Ä–º...")
        url = "https://bankinform.ru/bank/ipoteka"
        
        html = self.parse_with_retry(url, use_proxy=False)
        if not html:
            print("    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return 0
        
        soup = BeautifulSoup(html, 'html.parser')
        found = 0
        
        rows = soup.find_all('tr')
        
        for row in rows[1:21]:
            try:
                text = row.get_text()
                
                banks = self.extract_banks(text)
                if not banks:
                    continue
                
                bank_name = banks[0]
                rate = self.extract_rate(text)
                
                if bank_name and rate:
                    self.all_rates[bank_name] = rate
                    found += 1
                    print(f"      ‚úì {bank_name[:20]}: {rate}%")
                    
            except:
                continue
        
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {found}")
        return found
    
    # ===== –ò–°–¢–û–ß–ù–ò–ö 7: –Ø–Ω–¥–µ–∫—Å =====
    def parse_yandex_ru(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ø–Ω–¥–µ–∫—Å.–ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å"""
        print("  [7/10] –Ø–Ω–¥–µ–∫—Å...")
        url = "https://realty.yandex.ru/ipoteka/programs/"
        
        html = self.parse_with_retry(url, use_proxy=False)
        if not html:
            print("    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return 0
        
        soup = BeautifulSoup(html, 'html.parser')
        found = 0
        
        blocks = soup.find_all('div', class_=re.compile('program|card|item'))
        
        for block in blocks[:15]:
            try:
                text = block.get_text()
                
                banks = self.extract_banks(text)
                if not banks:
                    continue
                
                bank_name = banks[0]
                rate = self.extract_rate(text)
                
                if bank_name and rate:
                    self.all_rates[bank_name] = rate
                    found += 1
                    print(f"      ‚úì {bank_name[:20]}: {rate}%")
                    
            except:
                continue
        
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {found}")
        return found
    
    # ===== –ò–°–¢–û–ß–ù–ò–ö 8: –ú–ò–† –ö–≤–∞—Ä—Ç–∏—Ä =====
    def parse_mirkvartir_ru(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ú–ò–† –ö–≤–∞—Ä—Ç–∏—Ä"""
        print("  [8/10] –ú–ò–† –ö–≤–∞—Ä—Ç–∏—Ä...")
        url = "https://www.mirkvartir.ru/ipoteka/"
        
        html = self.parse_with_retry(url, use_proxy=False)
        if not html:
            print("    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return 0
        
        soup = BeautifulSoup(html, 'html.parser')
        found = 0
        
        items = soup.find_all('div', class_=re.compile('bank|item|rate'))
        
        for item in items[:15]:
            try:
                text = item.get_text()
                
                banks = self.extract_banks(text)
                if not banks:
                    continue
                
                bank_name = banks[0]
                rate = self.extract_rate(text)
                
                if bank_name and rate:
                    self.all_rates[bank_name] = rate
                    found += 1
                    print(f"      ‚úì {bank_name[:20]}: {rate}%")
                    
            except:
                continue
        
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {found}")
        return found
    
    # ===== –ò–°–¢–û–ß–ù–ò–ö 9: –¶–ò–ê–ù =====
    def parse_cian_ru(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¶–ò–ê–ù"""
        print("  [9/10] –¶–ò–ê–ù...")
        url = "https://www.cian.ru/ipoteka/programs/"
        
        html = self.parse_with_retry(url, use_proxy=False)
        if not html:
            print("    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return 0
        
        soup = BeautifulSoup(html, 'html.parser')
        found = 0
        
        blocks = soup.find_all('div', class_=re.compile('program|card|item'))
        
        for block in blocks[:15]:
            try:
                text = block.get_text()
                
                banks = self.extract_banks(text)
                if not banks:
                    continue
                
                bank_name = banks[0]
                rate = self.extract_rate(text)
                
                if bank_name and rate:
                    self.all_rates[bank_name] = rate
                    found += 1
                    print(f"      ‚úì {bank_name[:20]}: {rate}%")
                    
            except:
                continue
        
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {found}")
        return found
    
    # ===== –ò–°–¢–û–ß–ù–ò–ö 10: –î–æ–º–ö–ª–∏–∫ =====
    def parse_domclick_ru(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –î–æ–º–ö–ª–∏–∫"""
        print("  [10/10] –î–æ–º–ö–ª–∏–∫...")
        url = "https://ipoteka.domclick.ru/programs/"
        
        html = self.parse_with_retry(url, use_proxy=False)
        if not html:
            print("    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
            return 0
        
        soup = BeautifulSoup(html, 'html.parser')
        found = 0
        
        cards = soup.find_all('div', class_=re.compile('program|card|item'))
        
        for card in cards[:10]:
            try:
                text = card.get_text()
                
                banks = self.extract_banks(text)
                if not banks:
                    continue
                
                bank_name = banks[0]
                rate = self.extract_rate(text)
                
                if bank_name and rate:
                    self.all_rates[bank_name] = rate
                    found += 1
                    print(f"      ‚úì {bank_name[:20]}: {rate}%")
                    
            except:
                continue
        
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {found}")
        return found
    
    # ===== –û–¢–î–ï–õ–¨–ù–´–ï –ë–ê–ù–ö–ò =====
    def parse_individual_banks(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –±–∞–Ω–∫–æ–≤"""
        print("  –ü–∞—Ä—Å–∏–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –±–∞–Ω–∫–∏...")
        
        banks_to_parse = [
            ('–°–±–µ—Ä–±–∞–Ω–∫', 'https://www.sberbank.ru/ru/person/credits/home/buying_complete_house'),
            ('–í–¢–ë', 'https://www.vtb.ru/personal/ipoteka/'),
            ('–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫', 'https://alfabank.ru/get-money/mortgage/'),
            ('–¢-–ë–∞–Ω–∫', 'https://www.tbank.ru/ipoteka/'),
            ('–ì–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫', 'https://www.gazprombank.ru/personal/loans/mortgage/'),
            ('–†–æ—Å—Å–µ–ª—å—Ö–æ–∑–±–∞–Ω–∫', 'https://www.rshb.ru/loans/mortgage/'),
        ]
        
        for bank_name, url in banks_to_parse:
            try:
                html = self.parse_with_retry(url, use_proxy=False, retries=2)
                if html:
                    rate = self.extract_rate(html)
                    if rate:
                        self.all_rates[bank_name] = rate
                        print(f"    ‚úì {bank_name}: {rate}%")
                time.sleep(1)
            except:
                continue
    
    # ===== –ì–õ–ê–í–ù–´–ô –°–ë–û–† =====
    def collect_all_rates(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ 10 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        print("\n  üöÄ –ó–ê–ü–£–°–ö 10 –ò–°–¢–û–ß–ù–ò–ö–û–í")
        
        # –¢—è–∂–µ–ª–∞—è –∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—è (—Å –ø—Ä–æ–∫—Å–∏)
        self.parse_banki_ru()
        time.sleep(2)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã
        self.parse_sravni_ru()
        time.sleep(1)
        self.parse_mbk_ru()
        time.sleep(1)
        self.parse_vbr_ru()
        time.sleep(1)
        self.parse_finuslugi_ru()
        time.sleep(1)
        self.parse_bankinform_ru()
        time.sleep(1)
        self.parse_yandex_ru()
        time.sleep(1)
        self.parse_mirkvartir_ru()
        time.sleep(1)
        self.parse_cian_ru()
        time.sleep(1)
        self.parse_domclick_ru()
        time.sleep(1)
        
        # –û—Ç–¥–µ–ª—å–Ω—ã–µ –±–∞–Ω–∫–∏
        self.parse_individual_banks()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π
        normalized = {}
        name_mapping = {
            '—Å–±–µ—Ä': '–°–±–µ—Ä–±–∞–Ω–∫',
            '–≤—Ç–±': '–í–¢–ë',
            '–∞–ª—å—Ñ–∞': '–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫',
            '—Ç-–±–∞–Ω–∫': '–¢-–ë–∞–Ω–∫',
            '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ': '–¢-–ë–∞–Ω–∫',
            '–≥–∞–∑–ø—Ä–æ–º': '–ì–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫',
            '—Ä—Å—Ö–±': '–†–æ—Å—Å–µ–ª—å—Ö–æ–∑–±–∞–Ω–∫',
            '–ø—Ä–æ–º—Å–≤—è–∑—å': '–ü—Ä–æ–º—Å–≤—è–∑—å–±–∞–Ω–∫',
            '–ø—Å–±': '–ü—Ä–æ–º—Å–≤—è–∑—å–±–∞–Ω–∫',
            '—É—Ä–∞–ª—Å–∏–±': '–£—Ä–∞–ª—Å–∏–±',
            '–æ—Ç–∫—Ä—ã—Ç–∏–µ': '–ë–∞–Ω–∫ –û—Ç–∫—Ä—ã—Ç–∏–µ',
            '—Å–æ–≤–∫–æ–º': '–°–æ–≤–∫–æ–º–±–∞–Ω–∫',
            '–º—Ç—Å': '–ú–¢–° –ë–∞–Ω–∫',
            '–¥–æ–º.—Ä—Ñ': '–ë–∞–Ω–∫ –î–û–ú.–†–§',
            '–¥–æ–º—Ä—Ñ': '–ë–∞–Ω–∫ –î–û–ú.–†–§',
        }
        
        for raw_name, rate in self.all_rates.items():
            raw_lower = raw_name.lower()
            found = False
            
            for key, norm in name_mapping.items():
                if key in raw_lower:
                    if norm in normalized:
                        normalized[norm] = min(normalized[norm], rate)
                    else:
                        normalized[norm] = rate
                    found = True
                    break
            
            if not found:
                normalized[raw_name] = rate
        
        self.all_rates = normalized
        print(f"\n  ‚úÖ –í–°–ï–ì–û –£–ù–ò–ö–ê–õ–¨–ù–´–• –ë–ê–ù–ö–û–í: {len(self.all_rates)}")
        return self.all_rates

# ===== –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï =====
def format_message(rates_dict):
    if not rates_dict:
        return "üòî –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞–≤–∫–∏"
    
    rates_list = [(bank, rate) for bank, rate in rates_dict.items()]
    rates_list.sort(key=lambda x: x[1])
    
    top_rates = rates_list[:25]
    min_bank, min_rate = rates_list[0]
    
    text = f"""
üè† <b>–ò–ø–æ—Ç–µ–∫–∞ —Å–µ–≥–æ–¥–Ω—è: –ú–ò–ù–ò–ú–ê–õ–¨–ù–ê–Ø –°–¢–ê–í–ö–ê</b>

üî• <b>–õ—É—á—à–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:</b>
‚Ä¢ {min_bank} ‚Äî <b>{min_rate}%</b>

üìä <b>–¢–æ–ø-25 –±–∞–Ω–∫–æ–≤:</b>

"""
    
    for i, (bank, rate) in enumerate(top_rates, 1):
        if i == 1:
            text += f"ü•á {bank} ‚Äî {rate}%\n"
        elif i == 2:
            text += f"ü•à {bank} ‚Äî {rate}%\n"
        elif i == 3:
            text += f"ü•â {bank} ‚Äî {rate}%\n"
        else:
            text += f"‚Ä¢ {bank} ‚Äî {rate}%\n"
    
    text += f"""

üìÖ {datetime.now().strftime('%d.%m.%Y %H:%M')} (–ú–°–ö)
üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(rates_list)} –±–∞–Ω–∫–æ–≤
üîÑ –ò—Å—Ç–æ—á–Ω–∏–∫–∏: 10 –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–æ–≤ + –æ—Ç–¥–µ–ª—å–Ω—ã–µ –±–∞–Ω–∫–∏
"""
    
    return text

# ===== –û–¢–ü–†–ê–í–ö–ê =====
def send_to_channel(text):
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    data = {
        "chat_id": CHANNEL_ID,
        "text": text,
        "parse_mode": "HTML"
    }
    
    try:
        response = requests.post(url, data=data, timeout=10)
        if response.status_code == 200:
            print("  ‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ –∫–∞–Ω–∞–ª!")
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")

# ===== –ì–õ–ê–í–ù–ê–Ø =====
def main():
    print("=" * 60)
    print("üöÄ MEGA PARSER - 10 –ò–°–¢–û–ß–ù–ò–ö–û–í")
    print(f"üìÖ {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}")
    print("=" * 60)
    
    if not BOT_TOKEN or not CHANNEL_ID:
        print("‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫")
        return
    
    parser = MegaParser()
    rates = parser.collect_all_rates()
    
    print(f"\nüìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±–∞–Ω–∫–æ–≤: {len(rates)}")
    
    # –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø–∞—Å–Ω–æ–π —Å–ø–∏—Å–æ–∫
    if len(rates) < 10:
        print("‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø–∞—Å–Ω–æ–π —Å–ø–∏—Å–æ–∫...")
        fallback = {
            '–°–±–µ—Ä–±–∞–Ω–∫': 21.0,
            '–í–¢–ë': 20.1,
            '–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫': 20.5,
            '–¢-–ë–∞–Ω–∫': 16.9,
            '–ì–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫': 20.8,
            '–†–æ—Å—Å–µ–ª—å—Ö–æ–∑–±–∞–Ω–∫': 20.2,
            '–ü—Ä–æ–º—Å–≤—è–∑—å–±–∞–Ω–∫': 19.49,
            '–£—Ä–∞–ª—Å–∏–±': 18.79,
            '–ë–∞–Ω–∫ –û—Ç–∫—Ä—ã—Ç–∏–µ': 21.1,
            '–°–æ–≤–∫–æ–º–±–∞–Ω–∫': 20.9,
            '–ú–¢–° –ë–∞–Ω–∫': 20.7,
            '–ë–∞–Ω–∫ –î–û–ú.–†–§': 20.2,
            '–ë–∞–Ω–∫ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥': 18.49,
            '–¢—Ä–∞–Ω—Å–∫–∞–ø–∏—Ç–∞–ª–±–∞–Ω–∫': 20.25,
            '–í–ë–†–†': 20.4,
        }
        
        for bank, rate in fallback.items():
            if bank not in rates:
                rates[bank] = rate
    
    message = format_message(rates)
    send_to_channel(message)
    print("\n‚úÖ –ì–û–¢–û–í–û")

if __name__ == "__main__":
    main()